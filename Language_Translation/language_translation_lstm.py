# -*- coding: utf-8 -*-
"""Language_translation_LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p8FBB10FZZ0Y7ZFm4jmmN37tIKsSiQgs
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Embedding
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

"""Step 2: Dataset definition. This is the dataset where each tuple consists of a simpoke english phrase and its french translation. This is a small toy dataset for the purpose of demonstration

"""

data = [
    ("hello", "bonjour"),
    ("how are you", "comment ça va"),
    ("thank you", "merci"),
    ("good morning", "bonjour"),
    ("good night", "bonne nuit"),
    ("see you later", "à plus tard"),
    ("I love you", "je t'aime"),
]

"""Step 3: Text preparation zip(*data): Separates the data tuples into two separate lists: one for input_text(English)"""

input_texts, target_texts = zip(*data)

"""Step 4: tokenization
Tokenizer(): Creates a tokenizer that will convert text into sequence of integers.
fir_on_text(): This method crafts a vocabulary from the input_text and target_text and assigns a unique integer to each word.  
"""

input_tokenizer = Tokenizer()
target_tokenizer = Tokenizer()

input_tokenizer.fit_on_texts(input_texts)
target_tokenizer.fit_on_texts(target_texts)

"""test_to_sequences(): convert each text(sentence) into a sequence of integers. Each word in the text is replaced by its corresponding integer from the vocabulary"""

input_sequences = input_tokenizer.texts_to_sequences(input_texts)
target_sequences = target_tokenizer.texts_to_sequences(target_texts)

"""step : Vocabulary and sequence length calculation
word_index : This dictionary holds the integer mapping for each word. We add 1 to account for the 0-based indexing of sequences.
input_vocab_size and target_vocab_size: Store the size of the vocabulary for the input and target languages.
"""

input_vocab_size = len(input_tokenizer.word_index) + 1
target_vocab_size = len(target_tokenizer.word_index) + 1

"""max_input_len and max_target_len : Store the maximum length of sequence in the input and target languages respectively. This helps with padding the sequences to a uniform length"""

max_input_len = max(len(seq) for seq in input_sequences)
max_target_len = max(len(seq) for seq in target_sequences)

"""Step 6: Padding Sequences
pa_sequences(): Pads each sequence to ensure that all sequences have the same length. Padding is applied to the end of the sequences(padding='post')
"""

encoder_input_data = pad_sequences(input_sequences,maxlen = max_input_len, padding = "post")
decoder_input_data= pad_sequences(target_sequences,maxlen = max_target_len, padding="post")

"""One-hot encoding Target sequences
np.zeros(): Creates a zero matrix where each row corresponds to a sentence and each column corresponds to a time step in the sequence. The depth corresponds to the size of the vocabulary (for one hot-encoding).
for loop: Loops over the target sequences and creates one hot encoded vectors where only the index corresponding to the word is 1. This shift by 1 ensures that the target data starts predicting from the second word.
"""

decoder_target_data = np.zeros((len(target_texts), max_target_len, target_vocab_size), dtype="float32")
for i, seq in enumerate(target_sequences):
  for t, word in enumerate(seq):
    if t>0: #target sequence shifted by one
      decoder_target_data[i,t-1, word] = 1.0

"""Step 8: Splitting the Data
train_test_split(): Splits the input data (encoder and decoder inputs) and target data into training and testing sets.
test_size=0.2 means 20% of the data is used for testing and 80% for training.
"""

X_train,X_test,y_train,y_test, decoder_input_train, decoder_input_test = train_test_split(
    encoder_input_data, decoder_target_data, decoder_input_data, test_size = 0.2
)

"""Step 9: Model Architecture"""

# embedding_dim = 128 # or any other value you'd like, typically 50,100 or 300
# Define Hyperparameters
latent_dim = 128 #Number of units in LSTM
embedding_dim = 128 # Size of word embedding

"""Input(shape=(max_input_len,)): Defines the input shape for the encoder (input sentence length).
Embedding(): Maps the input word indices to dense vector of size embedding_dim. LSTM(): The LSTM layer processes the input embeddings ad returns two things: the final hidden state (state_h) and cell state(state_c). This states will be pass to the decoder.

"""

encoder_inputs = Input(shape = (max_input_len,))
encoder_embedding = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)
encoder_lstm = LSTM(latent_dim, return_state = True)
encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)

"""Similar to the encoder, the decoder also has an embedding layer followed by an LSTM. The LSTM recieves the encoder's final states (state_h, state_c) as initial states  for the decoing process.
return_sequences = True ensures that the decoder produces a sequence of outputs rather than just the last output.
"""

decoder_inputs = Input(shape=(max_target_len,))
decoder_embedding = Embedding (target_vocab_size, embedding_dim)(decoder_inputs)
decoder_lstm = LSTM(latent_dim, return_sequences = True, return_state = True)
decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state = [state_h, state_c])

"""Dense Layer : Generate actual output in probability
Dense(): A fully connected layer that outputs a probability distribution over the target vocabulary ( for each word in the sequence).
softmax: Ensures the output is probability distribution
"""

decoder_dense = Dense(target_vocab_size, activation = "softmax")
decoder_outputs = decoder_dense(decoder_outputs)

"""Step 10: Define the Model"""

model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer = "adam", loss = "categorical_crossentropy", metrics = ["accuracy"])

# Train the Model
model.fit([X_train, decoder_input_train], y_train, batch_size = 32, epochs = 100, validation_data = ([X_test, decoder_input_test], y_test))

# Purpose of interference models
# After the model has been trained, we need to define the inference process to actually generate translation.
# In the training process, both the encoder and decoder recieve complete sequences.
# However, during inference (prediction), we aonly have the input sequence, and decoder must generate the output word by word, one step at a time.
# Thus, we create two separate models for inference:

# Encoder Model: Converts the input sentence into the internal states(hidden and cell states) that are passed to the decoder.
# Decoder Model: Takes the encoder's internal states and generates the output sequence word by
# Define inference models for translation

# Encoder Model
encoder_model = Model(encoder_inputs,[state_h,state_c])

""" Purpose: The encoder processes  the input sequence and outputs its final internal states (hidden state state_h and cell state state_c).
 These states will be passed to the decoder during inference.
 encoder_inputs: The input sequence for the encoder (which is padded).
 [state_h, state_c]: The encoder's final states that the decoder will use to start generating the output sequence.

"""

# Decoder Model
decoder_state_input_h = Input(shape=(latent_dim,))
decoder_state_input_c = Input(shape=(latent_dim,))

"""decoder_state_input_h and decoder_state_input_c: Inputs to the decoder
This are the hidden state(state_h) and cell state(state_c)
 that were produced by the encoder.
 In inference, we don't have this states at the beginning, so they are taken as inputs for the decoder.
"""

decoder_lstm_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_embedding, initial_state = [decoder_state_input_h, decoder_state_input_c])

decoder_outputs = decoder_dense(decoder_lstm_outputs)
decoder_model = Model(
    [decoder_inputs, decoder_state_input_h, decoder_state_input_c],
    [decoder_outputs, decoder_state_h, decoder_state_c]
)

"""The decoder LSTM takes in the current word (embedded using the decoder_embedding layer) along with the hidden and cell states ( decoder_state_input_h and decoder_state_input_c) as initial states.
decoder_lstm_output: The LSTM output for the current time step (which represents the probabilities for each word in the vocabulary).
decoder_state_h, decoder_state_c: The updated hidden and cell states after processing the current word. these states will be passed back into the LSTM for teh next time step.

Function to decode a sequence using the the trained model
The function takes an input sequence (from a source language, for example) and uses na encoder-decoder model to generate a translated sequence (target language).
It  performs this in an iterative manner, predicting one word at a time, until it either predicts the end-of-sequence token or reaches a specified maximum length.
"""

def encode_sequence(input_seq):
  states_value = encoder_model.predict(input_seq)

"""input_seq: This is the sequence that you want to translate.
The encoder_model processes the input sequence and returns the state_value (hidden and cell states) that represent the context learned from the input sequence.
This states are used as a initial states
"""

target_seq = np.zeros((1,1))

"""target_seq: This starts as an array of zeros because at the beginning , there is no input to the decoder. As the decoder predicts word, this array will hold teh index of teh word generated at the previous state."""

stop_condition = False
decoded_sentence =''

"""decoded_sentence: An empty string that will hold the generated translation.

stop_condition: A flag to indicate when the decoding process should stop.

decoded_sentence: This string will store the predicted translation.
"""

#The decodeer LSTM takes in the corrent word (embedded using the decoder_embedding layer)
#along with the hidden and cell states(decoder_state_input_h and decoder_state_input_c)
#as initial states. Decoder_lstm_outputs: The LSTM output for the corrent time step
#(Which represents the probabilities for each word in the vocabulary).
#decoder_state_h,decoder_state_c : The updated hidden and cell states after processing the current word.
#These states will be passed back into the LSTM for the next time step.


#Function to decode a sequence using the trained modle
#Th efucntion takes an input sequnce (from a source language , for example)
#and uses an encoder-decoder model to generate a transkated sequence(target language).
#it performs this in a iterative manner, predicting one word at a time,
#until it either predicts the end-of-sequence token or reaches a specific maximum length.
def decode_sequence(input_seq):
  states_value = encoder_model.predict(input_seq)
#input_seq:This is the sequence that you eant to translate.
#The encoder_model prosses th einput sequence and returns the state_value
#(hidden ans cell states) that represents the context learned from the input sequnce.
#these states are used as the initial state fot the decoder

  target_seq = np.zeros((1,1))

#target_seq :this starts as an array of zeros because at the beginning,
#there is no input to the decoder. As the decoder predicts words,
#This array will hold th eindex of the word generated at the previous step.

  stop_condition = False
  decoded_sentence = ""

  #Decoded_sentence:an empty string that will hold the generated translation.
  #Stop_condition : A flag to indicate when the decoding process should stopp
  #decoded_senetence:This string will store the predicted translation

  while not stop_condition:
    #the loop continues until the translation is complete
    #(i.e.,when the decoder generates an end token or exceeds the allowed length).
    output_tokens,h,c = decoder_model.predict([target_seq] + states_value)
    #decoder_model uses the current target sequence(target_seq)
    #and the encoder's final states(states_value) to predict the next word.
    #output_tokes: the predicted probabilities of the next word.
    #h, c:the updated hidden and cell status. These states are passed to the next iteration to ensure continuity in generating coherent sentences
    sampled_token_index = np.argmax(output_tokens[0,-1:])
    sampled_word = target_tokenizer.index_word.get(sampled_token_index, "")
    #Output_tokens[0,-1,:]: The output_tokens array contains the predicted probabilities for each possible word in the vocal
    #the shape of output_token is typically (batch_size, sequence_length, vocabulary_size).
    #sequence_length is 1 because we are decoding one senetence.
    #vocabulary_size is the number pf possible words in the target vocabulary.
    #output_tokens[0, - 1, :] selects the predicted probabilities of word at the current time step from the vocabulary
    #illustation : Suppose the vocabulary has 5 words :{0: 'hello',1:'word',2:'how',3: 'are',4:'you'}
    #the outut_tokens might look something like this:
    #Output_tokens[0,-1,:] = [0.1,0.6,0.05,0.15,0.1]
    #sampled_token_index = np.argmax(output_tokens[0,-1,:]):

    #np.argmax() finds th eindex of the highest probabilitity from th eoutput_tokens array.
    #in this case, it will select the index 1 because the highest probability(0.6)
    #corresponds to the word 'world'
    #Now, using the sampled_token_index = 1:
    #sampled_word = "world"
    #putting it all together:
    #After runnning np.argmax(),the most likely words index(1 in this case) is seleceted .
    #This index is then used to retrieve the corresponding word("world" in this case)
    #from the tokenizer's dictionary.

    decoded_sentence += sampled_word + " "
    #The predicted word is appended to the decoded_senetence string.

    if sampled_word == "<end>" or len(decoded_sentence) > max_target_len:
      stop_condition = True
      #the decodinng process stops when the <end> token is predicted,
      #or if the setence exceeds the maximum allowed length(max_target_len).
      #Update the target sequence for the next ieteration:

    target_seq = np.zeros((1,1))
    #this line creates a 2D Numpy array filled with zeros, with the shape (1,1)
    #In the context of sequence-to-sequence models(such as machine translation)
    #this is used to hold the token(word index) that will be fed as input into the decoder
    #at the next time step

    target_seq[0,0] = sampled_token_index
    #target_seq[0,0] = sampled_token_index :
    #This line assigns the value of sampled_token_index (which is the index of the word predicted
    #by the decoder in the previous step) tot he target_seq.
    #The value is placed at position [0,0] because its a 1X1 array, and [0,0]
    #refers to the only element in the array.
    #samples_toke_index = 1(from the previous word prediction step).
    #After this assignemnt, the target_seq will look like this:
    #target_seq[0,0] = 1
    #result:target_seq = [[1.]]

    #purpose :
    #The target_seq is used as the input for the decoder at the next time step.
    #At each decoding step, the decoder needs to be fed the the token (or word) predicted
    #

    states_values = [h,c]

    return decoded_sentence

def translate(sentence):
  sequence = input_tokenizer.texts_to_sequences([sentence])
  sequence = pad_sequences(sequence,maxlen=max_input_len, padding = "post")
  translation = decode_sequence(sequence)
  return translation

translated_sentence = translate("hello")
print("Translated sentence : ", translated_sentence)